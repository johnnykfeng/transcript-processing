{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcript_processing_functions import \\\n",
    "                        full_transcript2essay, \\\n",
    "                        extract_metadata_as_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'./transcripts/test_transcript.md', 'r') as file:\n",
    "    raw_transcript = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 2-3 minutes to run\n",
    "final_essay = full_transcript2essay(raw_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models and fine-tuning were the main topics of a presentation given by Ehsan Kamalinejad. He began by discussing the rapid advancements in the field and the difficulty in choosing a specific topic to focus on. Ultimately, he decided to delve into the concept of fine-tuning large language models.\\n\\nKamalinejad provided a brief history of the development of large language models in the era of deep learning. He specifically focused on transformer-based models, which have seen significant development since 2018. These models, such as GPT and the family of models from Google, have been primarily used for auto-regressive tasks, where the models are trained to predict the next token in a sequence.\\n\\nThe pre-training of these models is done through self-supervised training, either through masked language modeling (MLM) or causal language modeling (CLM). Kamalinejad mentioned a disagreement with Amir regarding the terminology, as he referred to them as \"causal\" language models. Regardless, these models are trained on a wide range of text data, making them versatile for various tasks.\\n\\nThe pre-training process involves next token prediction, which is a simple yet effective setup. It allows the models to learn patterns and understand different contexts, such as mathematics or historical facts. Kamalinejad emphasized that these models excel at autoregressive tasks but require fine-tuning to be usable for specific domains or problems.\\n\\nTo extract the knowledge stored in these large language models, fine-tuning is necessary. There are two main methods of fine-tuning: supervised fine-tuning and reinforcement learning with human feedback. Supervised fine-tuning involves creating a diverse set of prompt datasets compatible with the task at hand. These prompts are completed by labelers, creating query-response pairs that are then used to fine-tune the model.\\n\\nReinforcement learning with human feedback, on the other hand, involves using other models or specific data to fine-tune the language model. Kamalinejad mentioned the RLS-chef method as an example of reinforcement fine-tuning. This method allows for the injection of biases into the model to improve its performance.\\n\\nIn the presentation, Ehsan Kamalinejad discussed the process of training large language models using reinforcement learning from human feedback (RLHF). He explained that in RLHF, prompts are created and exposed to the model, which generates responses. These responses are then ranked by human labelers based on their quality. The ranking mechanism is used to create a reward model, which helps identify better and worse responses.\\n\\nOnce the reward model is trained, it is applied to the base model during reinforcement learning. The reward model guides the base model by providing feedback on the generated text, encouraging good answers and discouraging bad ones. This training process is more complex than supervised fine-tuning, but it has shown to be more effective in reducing catastrophic forgetting, where the model forgets past knowledge.\\n\\nEhsan also mentioned that RLHF requires online data collection, as the model needs to generate responses for labeling. On the other hand, supervised fine-tuning allows for offline data collection, making it easier to gather large amounts of data from sources like Stack Overflow.\\n\\nDuring the presentation, Ehsan demonstrated the impact of supervised fine-tuning on model behavior using a live demo. He showed examples of a base model and a fine-tuned model answering questions. The fine-tuned model consistently provided more accurate and relevant answers, showcasing the effectiveness of fine-tuning in improving model performance.\\n\\nEhsan also discussed the pros and cons of supervised fine-tuning and RLHF. While RLHF requires more effort from labelers, it offers a more robust and less prone to catastrophic forgetting training approach. Supervised fine-tuning, on the other hand, allows for easier data collection but may suffer from catastrophic forgetting.\\n\\nThe presentation concluded with a discussion of the amount of data required for supervised fine-tuning and RLHF. Ehsan mentioned that the data requirements depend on the specific task and model, and it is not clear which approach works better in all cases.\\n\\nIn the presentation, Ehsan Kamalinejad discusses various aspects of large language models and machine learning research. He begins by addressing the question of how much data is needed for these models to generate good answers in a specific domain. While there is no definitive answer yet, empirical results suggest that tens of thousands of samples are required for effective fine-tuning of language models. This is relatively small compared to the large datasets typically used to train deep neural network models.\\n\\nAnother question raised is the extent of improvement achieved through fine-tuning. Currently, there is no agreed-upon measure to assess the quality of these models, as traditional benchmarks are becoming saturated. Therefore, there is a need for better metrics to evaluate the performance of large language models.\\n\\nThe topic of scaling laws is also discussed, particularly in terms of limited compute resources. It is important to determine whether to allocate limited compute power to obtaining more tokens (larger dataset) or using a larger model. The Chinchilla paper from DeepMind provides insights into scaling laws, but the question of fine-tuning is still significant and requires further research.\\n\\nThe effectiveness of methods such as Path in parameter-efficient fine-tuning is another important question. While some partial answers exist, there is still much to be understood about these methods and how they compare to each other.\\n\\nDuring the presentation, Amir Feizpour asks about the specific parameters that are changed during fine-tuning. Ehsan explains that there are different methods, such as Path and Laura, which involve freezing certain layers or weights of the model while fine-tuning the rest. Each method has its own strengths and weaknesses, and it is currently unclear which one is the most effective.\\n\\nAmir also asks about practical tips for using these techniques at home, particularly for smaller-scale projects. Ehsan suggests that while training large models on personal machines may be challenging, it is possible to shrink the models by freezing parameters and training them on GPUs available in platforms like Colab. He mentions that Hugging Face provides resources and documentation on how to train models with limited compute.\\n\\nThe discussion concludes with the mention of open-source models and datasets, such as OpenAI\\'s Open Assistant effort, which can be used for supervised fine-tuning or other methods. Ehsan highlights that these methods are model-agnostic, allowing researchers to apply them based on the available compute resources.\\n\\nIn the presentation, Ehsan Kamalinejad discussed large language models and machine learning research. He mentioned that there are different sizes of models available, ranging from 1 billion to 60 billion parameters. These models can be run on laptops or on cloud platforms like AWS. Kamalinejad also mentioned that he could help with AWS if anyone needed assistance.\\n\\nDuring the presentation, Amir Feizpour asked about tools to collect data specifically for prompt and answer pairs to train their agent. Kamalinejad mentioned that there are several labeling tools available, with Scale AI being the best paid option. He also mentioned that there are some open-source tools, but their quality and interaction are not as good as Scale AI. Kamalinejad mentioned that Open Assistant and an upcoming tool from AWS could be options for those looking for free labeling tools.\\n\\nFeizpour then brought up the idea of using large language models for data generation, specifically synthetic data generation. He asked if it was possible to collect enough data, use the OpenAI API to generate a training set, and train a small model on that data. Kamalinejad confirmed that it is possible to do so, but mentioned that there are licensing and legal issues to consider. OpenAI does not allow the use of their API and interactive application for training your own models. Kamalinejad explained that while it is possible to distill knowledge from OpenAI\\'s models to your own, it may not be permissible to release the trained model.\\n\\nThe discussion then shifted to the fine-tuning paradigm and its accessibility. Kamalinejad emphasized the importance of sticking to principles and mentioned that methods like applying reinforcement learning (RL) for fine-tuning are generic and can be applied to any model. He also mentioned other fine-tuning methods like supervised fine-tuning and prompt tuning, but noted that they may be more specific and less generalizable. Kamalinejad stated that RL is currently the most principled approach for fine-tuning, but acknowledged that it can be complex on the engineering side.\\n\\nFeizpour added that data is ultimately the most important aspect of machine learning models. He emphasized that data is the gold standard and that algorithms and frameworks are not particularly defensible. Kamalinejad agreed and highlighted the difficulty of obtaining the right data for training models.\\n\\nTowards the end of the presentation, Kamalinejad mentioned that AWS has a new initiative to create foundational models and collaborate with other labs to make open-source models more widely available. He mentioned collaborations with companies like Cohere, Tropic, and Stability AI. Kamalinejad also mentioned ongoing discussions with Entropic and Stability from AWS.\\n\\nOverall, the presentation provided insights into large language models, fine-tuning paradigms, data collection tools, and the future initiatives of AWS in the field of machine learning research.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_filepath = r'./essays/test_essay.txt'\n",
    "\n",
    "# save the final essay to a file\n",
    "with open(essay_filepath, 'w') as file:\n",
    "    file.write(final_essay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start here if essay exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcript_processing_functions import \\\n",
    "                        full_transcript2essay, \\\n",
    "                        extract_metadata_as_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load essay from file\n",
    "essay_filepath = r'./essays/test_essay.txt'\n",
    "with open(essay_filepath, 'r') as file:\n",
    "    essay = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 seconds\n",
    "metadata = extract_metadata_as_json(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'Title': 'Large Language Models and Fine-Tuning', 'Speaker': 'Ehsan Kamalinejad', 'Summary': \"Ehsan Kamalinejad discusses large language models, focusing on fine-tuning and its methods. He explores supervised fine-tuning and reinforcement learning with human feedback (RLHF). Kamalinejad also addresses the amount of data required for effective fine-tuning and the need for better evaluation metrics. The presentation covers topics such as scaling laws, parameter-efficient fine-tuning, and practical tips for smaller-scale projects. Kamalinejad mentions open-source models and datasets, as well as tools for data collection and synthetic data generation. The discussion concludes with insights into AWS's initiatives in the field of machine learning research.\", 'Topics': ['Large language models', 'Fine-tuning', 'Supervised fine-tuning', 'Reinforcement learning with human feedback (RLHF)', 'Data requirements', 'Evaluation metrics', 'Scaling laws', 'Parameter-efficient fine-tuning', 'Practical tips', 'Open-source models and datasets', 'Data collection tools', 'Synthetic data generation', 'AWS initiatives'], 'Takeaways': ['Fine-tuning is necessary to extract knowledge from large language models', 'Supervised fine-tuning involves creating prompt datasets and query-response pairs', 'RLHF uses human feedback to rank model responses and guide reinforcement learning', 'RLHF reduces catastrophic forgetting in models', 'Data requirements for fine-tuning depend on the task and model', 'Better evaluation metrics are needed for large language models', 'Scaling laws and parameter-efficient fine-tuning are important research areas', 'Practical tips include shrinking models and utilizing available compute resources', 'Open-source models and datasets are available for supervised fine-tuning', 'Data collection tools like Scale AI can assist in creating prompt and answer pairs', 'Synthetic data generation is possible but has licensing and legal considerations', 'RL is a generic and principled approach for fine-tuning', 'Data is crucial for machine learning models', 'AWS has initiatives for creating foundational models and collaborating with other labs']}\n"
     ]
    }
   ],
   "source": [
    "# print(type(metadata))\n",
    "# print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Large Language Models and Fine-Tuning\n",
      "Speaker: Ehsan Kamalinejad\n",
      "Summary: Ehsan Kamalinejad discusses large language models, fine-tuning paradigms, data collection tools, and the future initiatives of AWS in the field of machine learning research.\n",
      "Topics: [{'Topic': 'Development of Large Language Models', 'Takeaways': ['Transformer-based models have seen significant development since 2018', \"Models like GPT and Google's family of models are primarily used for auto-regressive tasks\", 'Pre-training involves self-supervised training through masked language modeling or causal language modeling']}, {'Topic': 'Fine-Tuning Large Language Models', 'Takeaways': ['Fine-tuning is necessary to extract knowledge from large language models', 'Two main methods of fine-tuning: supervised fine-tuning and reinforcement learning with human feedback', 'Supervised fine-tuning involves creating prompt datasets and query-response pairs', 'Reinforcement learning with human feedback involves ranking responses by human labelers', 'RLHF helps reduce catastrophic forgetting']}, {'Topic': 'Training Large Language Models', 'Takeaways': ['RLHF requires online data collection, while supervised fine-tuning allows for offline data collection', 'Supervised fine-tuning is effective for improving model performance', 'RLHF offers a more robust training approach but requires more effort from labelers']}, {'Topic': 'Data Requirements and Evaluation', 'Takeaways': ['Tens of thousands of samples are required for effective fine-tuning', 'Better metrics are needed to evaluate the performance of large language models', 'Determining data allocation for more tokens or larger models is important', 'Path and Laura methods involve freezing certain layers or weights during fine-tuning']}, {'Topic': 'Practical Tips and Tools', 'Takeaways': ['Training large models on personal machines can be challenging, but shrinking models is possible', 'Hugging Face provides resources for training models with limited compute', 'Open-source models and datasets like Open Assistant can be used for fine-tuning', 'Scale AI is a paid labeling tool, while Open Assistant and upcoming AWS tool are free options']}, {'Topic': 'Using Large Language Models for Data Generation', 'Takeaways': ['Using OpenAI API for data generation has licensing and legal issues', \"Distilling knowledge from OpenAI's models is possible, but releasing trained models may not be permissible\"]}, {'Topic': 'Fine-Tuning Paradigm and Accessibility', 'Takeaways': ['Reinforcement learning (RL) is a generic and principled approach for fine-tuning', 'Supervised fine-tuning and prompt tuning are more specific methods', 'Data is the most important aspect of machine learning models', 'AWS is collaborating with other labs to make open-source models more widely available']}]\n"
     ]
    }
   ],
   "source": [
    "for key, value in metadata.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata_filepath = r'./metadata/test_metadata2.json'\n",
    "with open(metadata_filepath, 'w') as file:\n",
    "    json.dump(metadata, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON to RST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcript_processing_functions import \\\n",
    "                        full_transcript2essay, \\\n",
    "                        extract_metadata_as_json, \\\n",
    "                        dict_to_rst, \\\n",
    "                        write_dict_to_rst\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json from file\n",
    "json_file = r'./metadata/test_metadata2.json'\n",
    "with open(json_file, 'r') as file:\n",
    "    json_metadata = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Title\": \"Large Language Models and Fine-Tuning\", \"Speaker\": \"Ehsan Kamalinejad\", \"Summary\": \"Ehsan Kamalinejad discusses large language models, fine-tuning paradigms, data collection tools, and the future initiatives of AWS in the field of machine learning research.\", \"Topics\": [{\"Topic\": \"Development of Large Language Models\", \"Takeaways\": [\"Transformer-based models have seen significant development since 2018\", \"Models like GPT and Google's family of models are primarily used for auto-regressive tasks\", \"Pre-training involves self-supervised training through masked language modeling or causal language modeling\"]}, \n",
      "{\"Topic\": \"Fine-Tuning Large Language Models\", \n",
      "\"Takeaways\": [\"Fine-tuning is necessary to extract knowledge from large language models\", \n",
      "\"Two main methods of fine-tuning: supervised fine-tuning and reinforcement learning with human feedback\", \n",
      "\"Supervised fine-tuning involves creating prompt datasets and query-response pairs\", \n",
      "\"Reinforcement learning with human feedback involves ranking responses by human labelers\", \n",
      "\"RLHF helps reduce catastrophic forgetting\"]}, \n",
      "{\"Topic\": \"Training Large Language Models\", \"Takeaways\": [\"RLHF requires online data collection, while supervised fine-tuning allows for offline data collection\", \"Supervised fine-tuning is effective for improving model performance\", \"RLHF offers a more robust training approach but requires more effort from labelers\"]}, \n",
      "{\"Topic\": \"Data Requirements and Evaluation\", \"Takeaways\": [\"Tens of thousands of samples are required for effective fine-tuning\", \"Better metrics are needed to evaluate the performance of large language models\", \"Determining data allocation for more tokens or larger models is important\", \"Path and Laura methods involve freezing certain layers or weights during fine-tuning\"]}, {\"Topic\": \"Practical Tips and Tools\", \"Takeaways\": [\"Training large models on personal machines can be challenging, but shrinking models is possible\", \"Hugging Face provides resources for training models with limited compute\", \"Open-source models and datasets like Open Assistant can be used for fine-tuning\", \"Scale AI is a paid labeling tool, while Open Assistant and upcoming AWS tool are free options\"]}, {\"Topic\": \"Using Large Language Models for Data Generation\", \"Takeaways\": [\"Using OpenAI API for data generation has licensing and legal issues\", \"Distilling knowledge from OpenAI's models is possible, but releasing trained models may not be permissible\"]}, {\"Topic\": \"Fine-Tuning Paradigm and Accessibility\", \"Takeaways\": [\"Reinforcement learning (RL) is a generic and principled approach for fine-tuning\", \"Supervised fine-tuning and prompt tuning are more specific methods\", \"Data is the most important aspect of machine learning models\", \"AWS is collaborating with other labs to make open-source models more widely available\"]}]}\n"
     ]
    }
   ],
   "source": [
    "print(dict_to_rst(json_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_rst(json_metadata, r'./metadata/test_metadata2.rst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2rst(metadata, rst_filepath):\n",
    "  if not isinstance(metadata, dict):\n",
    "      metadata = json.loads(metadata)\n",
    "  \n",
    "  # rst_filepath = './essays/test.rst'\n",
    "  with open(rst_filepath, 'a') as the_file:\n",
    "      the_file.write(\"\\n\\n\")\n",
    "      for key, value in metadata.items():\n",
    "          if key == \"Title\":\n",
    "              title_mark = \"=\" * len(f'{value}')\n",
    "              the_file.write(title_mark + '\\n')\n",
    "              the_file.write(f\"{value} \\n\")\n",
    "              the_file.write(title_mark + '\\n')\n",
    "          elif key == \"Speaker\":\n",
    "              the_file.write('*' + f\"{value}\" + '* \\n\\n')\n",
    "          elif key == \"Summary\":\n",
    "              title_mark = '-' * len(f'{key}')\n",
    "              the_file.write(\"Summary \\n\")\n",
    "              the_file.write(title_mark + '\\n')\n",
    "              the_file.write(f\"{value} \\n\\n\")\n",
    "          elif key == \"Topics\":\n",
    "              the_file.write(\"Topics: \\n\")\n",
    "              the_file.write(title_mark + '\\n')\n",
    "              for topic in value:\n",
    "                  the_file.write(\"\\t\" + f\"{topic['Topic']} \\n\")\n",
    "                  for takeaway in topic['Takeaways']:\n",
    "                      the_file.write(\"\\t\\t\" + f\"* {takeaway} \\n\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "json2rst(json_metadata, r'./metadata/test.rst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eeq-qdde-agt (2023-03-17 12:11 GMT-4) - Transcript\n",
      "Attendees\n",
      "Ali Mirzaei, Amir Feizpour, Ammar Khan, Debjani Mukherjee, Dikshya Mohanty, Karim Khayrat, Karim Khayrat's Presentation, Kua Chen, Marzieh Zare, mohammed fahad, Nikhil Varghese, Percy Chen, Suhas Pai, You Cheng, Yujing Yang\n",
      "Transcript\n",
      "This editable transcript was computer generated and might contain errors. People can also change the text after it was created.\n",
      "Karim Khayrat: So yeah, I'm gonna talk about two things today. I'm going to talk about Self-instructs next and but first, we'll start with any bars. Which is a paper on. On few short learning for information retrieval tasks. I'll it imparts us for inquisitive. Parrots for search, I guess like,\n",
      "Karim Khayrat:  Yeah, in in the sense that parrot, repeat you know what you say. So we'll see if the paper matches the name. So yeah, what's a was a challenge that paper aims to solve is that you guys know it's expensive to gather data for the train retrieval models in particular. It's expensive to gather human data. And and the solution that they presented the few short learning approach for generating, synthetic training data sets. So in summary, this is the model overview.\n",
      "Karim Khayrat:  a Given a document, let's say, you know, a paragraph or some piece of tech And a prefix, which is basically the prompt right? That consists of N, pairs of questions and relevant documents. So you go let's say to charge GPT you give it documents and some questions really to that document and give it, let's say three pairs. That's what you use in the paper there. Randomly chosen from a data sets from the Marco Ms. Marco data set. And that creates basically a prefix. In the prompt. And then you use a User much better, language model. And what you're trying to find tune To generate a question. That's relevant to document, that's the input, the new document that you want to create questions for.\n",
      "Karim Khayrat:  so, we will come to an example after this, but, but basically, you if you set n equal to three you can possibly generate thousands if not millions of training examples, From a random document collection. and after you generate, after the step where you generating so many synthetic questions to your documents,\n",
      "Karim Khayrat:  Not all of them, of course, are going to be of equal quality, right? So what they do is they filter some top k. In this case, the filtered They generated 100,000 and then you filter the top 10,000. Based on the log probability of the generated output. So you know, in a transformer model every every output has some probability. So, based on the lot, the largest log probability of the output and the they took the filter them and they took the top 10,000 examples.\n",
      "Karim Khayrat:  So this is basically model overview. You have a few short, input three examples for documents D. This is Document D. But they give it. And the examples are not one in the in this image. So you have some or they're shown behind, no, actually, they're not going in this image, the way they presented it. So, you have the document, we don't know a lot about the effects of caffeine during pregnancy on you or your baby. So it's best to limit the amount you get each day. This could be like from a web in the website or or any other website, for example, and you're asking the language model To give a question on this document, right? So you input this to a language model G And it recreates some questions like What are the effects of caffeine during pregnancy and there's a probability of this question being generated, right? And so you connect these generated question document pairs\n",
      "Karim Khayrat:  and you select the top key based on the probability. Of the generative question. And then you get some training pairs and then you input these training pairs to fine tune a re-ranker, right? So what I read anchor does is it takes the question and the document and it scores from zero to one in the paper you say it's a classification model. So I assume the use same structures classification model to to basically classify whether this question and document match are there relevant or not. So this is the relevant SEA score. so, relevancy, given of given relevancy, given the document and question, Okay.\n",
      "00:05:00\n",
      "Karim Khayrat:  So I guess there's a question.\n",
      "Amir Feizpour: Yeah, I can ask a question. So that the three examples are they just?\n",
      "Karim Khayrat:  Yeah.\n",
      "Amir Feizpour: Like what you're showing is just a very bottom of The prompt I assume that there is instruction and…\n",
      "Karim Khayrat:  Yes.\n",
      "Amir Feizpour: there are three examples, and then what you're showing in blue is what came after.\n",
      "Karim Khayrat:  Yes, exactly. Yeah.\n",
      "Amir Feizpour:  Okay. and, What's the setup for the language model G? So, is it producing only one question is producing several questions. The talk about that at all.\n",
      "Karim Khayrat: Um, let me I I believe you you can, you can produce as many questions for document. As you want. I'm not I don't remember exactly the details of of what they did. But yeah in principally you you generate multiple questions? Because also the prompt that the sample, three examples, right? Those changes. So you can have Different. Different questions based on the prompt based on the three examples that you give it previously and…\n",
      "Amir Feizpour: Okay, are the examples?\n",
      "Karim Khayrat: you sample three different ones each time.\n",
      "Amir Feizpour: Are the three examples? Chosen? Dynamically or are those like heart condition.\n",
      "Karim Khayrat:  I don't think they're fixed. I think there's children dynamically. I think the fixed number that they talk about is three. Like, three examples that that would make a more sense like the number three effects. But the examples themselves are are sampled.\n",
      "Amir Feizpour: Place and…\n",
      "Karim Khayrat:  And they are sampled from them from the Marco data set.\n",
      "Amir Feizpour: I assume it's not.\n",
      "Amir Feizpour:  I see, are they randomly sampled or like, is there a retrieval happening there?\n",
      "Karim Khayrat:  No, I believe they're randomly sample because there's what will there are no reason to retrieve it on something particular at this point. So there are generating question document pairs, right? So there's no retrievals you have a document randomly sampled And you just,\n",
      "Karim Khayrat: of course, like they talk about things about, let's say, if you, if you do domain specific, Domain specific sampling for documents, right? But they don't, they don't support that in their conclusion, that you should. You should go that route,…\n",
      "Amir Feizpour: Presentation, and…\n",
      "Karim Khayrat: you know?\n",
      "Amir Feizpour: the re-ranker. What's the purpose of it? I didn't quite follow. There is that to choose the question. So, the selection of the top K has happened already. So reruncle is being.\n",
      "Karim Khayrat:  Right. Thank you. Yes exactly. They're fine tuning the the data set I guess this is this is, for example.\n",
      "Karim Khayrat:  Let me see. Variation. Here you go. You the randomly a hundred thousand documents. From from using this process. Like the The Q comma DPs. and, Degenerate. Yeah, here you go. The generate only one question per document. That that answer your question. But in principle, you can generate as many questions as you want. So it's not like\n",
      "Karim Khayrat: Part of this. This. Yeah, it's not like Important. I don't think it's a very important detail but here they generate one question for document. Using the Gpt's query and the language model. And then the Select 10,000 pairs. You'll find you. And then some detail I missed out Skipton, is that, yeah, you are printing a classifier, so they need negative examples. So, what they do is they just use B, m, twenty five on on the document. At it. So they have a question, right? So the they can just query a document using bm25. I think the select like 100 documents so there and…\n",
      "Nikhil Varghese: You.\n",
      "Karim Khayrat: then they select one random one of them. So it's like a bad match. Let's say.\n",
      "Karim Khayrat:  I ran them random sample and and they use that as a negative sample.\n",
      "Amir Feizpour: So what is the classifier you mentioned?\n",
      "Karim Khayrat:  The classifier is there is the rear anchor. The classifier, the reactor takes the document and a query. And it it outputs whether they're relevant or not.\n",
      "00:10:00\n",
      "Amir Feizpour: Your audio is cutting out.\n",
      "Karim Khayrat:  but,\n",
      "Amir Feizpour: We cannot hear you can.\n",
      "Karim Khayrat: Can you hear me now? Hello. Yes.\n",
      "Amir Feizpour: Yes, yes.\n",
      "Karim Khayrat: Oh yeah.\n",
      "Karim Khayrat:  Yeah, so so, where did I cut off? It did I cover the negative examples? Part?\n",
      "Amir Feizpour: a yeah, like the like you moved to the next slide so here\n",
      "Karim Khayrat:  Yeah, so so this is just an example of Of a prompt. If you if you you have like a document, the relevant query, which is generated by gp3 and and example, two an example, three,\n",
      "Karim Khayrat:  Are you? This is this is the prompt. This is a question.\n",
      "Karim Khayrat: this is an example of the few short input that's given to To the to the query.\n",
      "Amir Feizpour: Thank you.\n",
      "Karim Khayrat: Yeah. So GPT 3 now takes this. and,\n",
      "Amir Feizpour: You.\n",
      "Nikhil Varghese: so, what's Like I icon quite understand why,…\n",
      "Karim Khayrat:  Butter.\n",
      "Nikhil Varghese: what's that? Good question. And a bad question.\n",
      "Karim Khayrat: Okay. What the?\n",
      "Nikhil Varghese:  It's the line doesn't seem very far.\n",
      "Karim Khayrat: Yeah. So these are two methods in which they prompted gpt3. So the left side is is the standard one that you expect, but then they tried having a good question. And a bad question means a good question, is a good question. And a bad question is some is one. That's not very relevant to the document. and,\n",
      "Karim Khayrat:  They try to. Have and then charge the PT. Just generates the good question but it's more like It, it knows what the bad question and what's a good question. So it can differentiate between generating a good question and a bad question but ideas to generate higher quality questions. Let's say using this kind of prompt.\n",
      "Karim Khayrat:  and,\n",
      "Karim Khayrat:  Yeah.\n",
      "Amir Feizpour: You want to thank you. So,\n",
      "Karim Khayrat: And and they will get to the to the detail to the to the results later like whether this method got got anything improved, anything or not. But yeah,…\n",
      "Amir Feizpour: so,\n",
      "Karim Khayrat: that that was the idea between behind it.\n",
      "Nikhil Varghese: Yeah, don't matter.\n",
      "Amir Feizpour:  so I, I\n",
      "Nikhil Varghese: Seems The method seems reasonable. but, the questions over here, maybe the model is both models are not too bad. So it's hard to really say. One is a clear winner.\n",
      "Amir Feizpour: so I I'm a little So the left side are the questions from Barcode Edison.\n",
      "Amir Feizpour:  If you're saying anything, we cannot hear you.\n",
      "Karim Khayrat: Yeah, I don't. yeah, so so they have yes, they have\n",
      "Karim Khayrat: No, they don't they don't have the start with.\n",
      "Karim Khayrat:  With. Human examples. Let me pull up the paper just to make sure that I got this right.\n",
      "Amir Feizpour: Right. Because You know, I'm also trying to wrap my head around. What is a good question and what is about question?\n",
      "Karim Khayrat: Okay.\n",
      "Nikhil Varghese: It feels more like. Being more specific I guess. Versus a more vague question.\n",
      "Amir Feizpour: Yeah. Actually, you know, or another way that seems. To me, that might be the definition of good question. Like A lot of what they're saying. Bad questions. They have like single word answers, but the quote unquote. Good questions have probably long format answers. That might also be, you know, the difference.\n",
      "Nikhil Varghese:  You.\n",
      "Amir Feizpour: well, my question is Where did these questions come from? Is it from the market answer? For\n",
      "00:15:00\n",
      "Karim Khayrat:  Where did the good and bad questions come from, right?\n",
      "Amir Feizpour: You know, that's a\n",
      "Karim Khayrat:  Okay, so yeah. I got yeah I got I got the part where the, the talk about the bad questions. For for the second prompt template called Guided by Bad Questions gbq. Which is the, which is illustrated on the right? The on that. On this, on the current slide.\n",
      "Karim Khayrat:  It's it's similar to the vanilla template, but we encourage the language model, contextual aware questions, then the ones from Ms, Marco for that, we use that Marco questions as examples of bad questions and manually. It once as good questions. So rather than finding the answer and part of the input document, the full context of the document will contribute to the answer. With this prompt the model generator good and a bad question for each document. You\n",
      "Karim Khayrat:  Because the gpq prompt by design generates questions. There are different from MS. Marker ones. We use a vanilla prompt to generate questions that are you? Oh yeah.\n",
      "Karim Khayrat:  As they use Ms Markle. Yes.\n",
      "Amir Feizpour: Uh, Karim. Is there a way to fix your audio problem? Because you're cutting out a lot and it's barely understandable.\n",
      "Karim Khayrat: maybe I'll just call then on On Ali,…\n",
      "Amir Feizpour: Don't you? I'm guessing that it is your headphone not your connection.\n",
      "Karim Khayrat: I use my phone.\n",
      "Amir Feizpour: Is there another headphone you can use or you? Maybe talk about that?\n",
      "Karim Khayrat:  Yeah.\n",
      "Percy Chen: I think you are charities for.\n",
      "Amir Feizpour: Smart, was that?\n",
      "Karim Khayrat: Yeah.\n",
      "Percy Chen: Are you a trail? Is his phone?\n",
      "Karim Khayrat:  Yeah. Sorry about that, and joining with my phone. Now, so, That should.\n",
      "Amir Feizpour: You know, whatever you\n",
      "Amir Feizpour: You. Okay, yeah. Keep going.\n",
      "Karim Khayrat: Okay, cool. Yeah. So\n",
      "Karim Khayrat: all for, for, Question Madison.\n",
      "Amir Feizpour: You. Yeah, okay. So to summarize, then they're using the questions from the market data set as bad questions. And they are essentially writing. The good questions themselves, just trying to make them more complicated.\n",
      "Karim Khayrat: That exact. so, in this case, Supervised. Because really re The. I guess. This. You don't they don't need a lot of do. It's like you need a lot of examples. Was. just, Each time.\n",
      "Karim Khayrat: and,\n",
      "Karim Khayrat: Yeah, so this is this is the result.\n",
      "Karim Khayrat: Have. On the market data sets.\n",
      "Karim Khayrat: On the track ideas and listen data sets. And with different metrics. so,\n",
      "Karim Khayrat: for, for A unsupervised version with.\n",
      "00:20:00\n",
      "Karim Khayrat: Without without the\n",
      "Karim Khayrat: They have.\n",
      "Karim Khayrat: State-of. This time. So of course, this paper was was published Quite some time ago. Of aliens.\n",
      "Karim Khayrat: and, And they have. Or they use them the mono T5.\n",
      "Karim Khayrat: Different parameters to 20 million parameters and 3 billion parameters. And they compared that. To. Davinci Curie and other from, from the Openai.\n",
      "Karim Khayrat: opening a you provide the API for and, Yeah, their metrics one of them, this case.\n",
      "Amir Feizpour: Books. Sorry,…\n",
      "Karim Khayrat: An achievement meth. Evil meth.\n",
      "Amir Feizpour: can I Can ask a big question, I'm not sure what I'm looking at. So,\n",
      "Karim Khayrat: You.\n",
      "Karim Khayrat: oh, Yes. So On like the Mon. Over here is the River. Already. so the the inference, the inference stage for the retrieval is first Actually. A bag of. It was with BM. Life.\n",
      "Karim Khayrat: it's I related to 100 documents, but I'm not. remember Ex, Remember.\n",
      "Karim Khayrat: the Mon I Dec. One. and, Mod. Equal number of, Examples in their frontier.\n",
      "Karim Khayrat: Published the State.\n",
      "Karim Khayrat: Which is.\n",
      "Karim Khayrat: You. and,\n",
      "Karim Khayrat: Since generating. Degrees. and, The. The.\n",
      "Karim Khayrat: It for the courage. just, Of. Is from.\n",
      "Karim Khayrat: and, The Great Document.\n",
      "Karim Khayrat: Again. is instead of\n",
      "Karim Khayrat: Was.\n",
      "Karim Khayrat: And the score of Monotypically So it's,…\n",
      "Amir Feizpour: You carry,…\n",
      "Karim Khayrat: Monotypically directly. it's…\n",
      "Amir Feizpour: sorry. Hold on. Hold on.\n",
      "Karim Khayrat: Yeah.\n",
      "Amir Feizpour: It's impossible to. hear what you were saying, there is still\n",
      "Karim Khayrat: okay, so I I left,…\n",
      "Amir Feizpour: oh, That's much better,…\n",
      "Karim Khayrat: hello, can you\n",
      "Amir Feizpour: I don't know what changed, but yeah, that's much better.\n",
      "Karim Khayrat:  I changed, I changed from my Wi-Fi to to 3G. Hopefully, that fixes things. Sorry.\n",
      "Amir Feizpour: Yeah, your your home Wi-Fi sucks. You need to change that.\n",
      "Karim Khayrat: If it's been I'm sorry but yeah, don't don't, don't go to bed. Don't go to Rogers, don't go to anything here. Okay so sorry version, two of the language model instead of the opening eye, Curie model the use and gptj for for generating the synthetic query. So the they move to an open source model and,\n",
      "Karim Khayrat:  The GPG. Pro is prompted with three examples from Ms, Marco? and,\n",
      "Karim Khayrat:  Instead of filtering. By by probability, they filtered by mono, T5 model. Until? let's say the standard one just to\n",
      "Karim Khayrat:  get a score from it. And of course, this is more expensive. This step would probably be more expensive than than filtering by probability. But at the end of the day, if you, if you have the hardware to run this fine tuning, this step should be relatively be cheap. And since you're you're only doing this once to generate the synthetic data and then a mono T5 3B is fine-tuned on the synthetic data and here they they compare. You know. At the mono T5. Trained on Marco alone. Trained on Marco plus in parts V1. And sorry, fine tunes and on Marco plus in in parts version one, which is the previous paper.\n",
      "00:25:00\n",
      "Karim Khayrat:  And the version 2, which is the updated one. And the good thing is that the scores don't change by much. So an average, it's 0.539 versus 0.545. And in this step, the\n",
      "Karim Khayrat:  In this step, of course they they used everything is open source so they didn't rely on the open AI API at all over here. but, but I would criticize this part is because in version 2, They use this query document pair filtering, not based on the probability, but based on on filtering with the model itself.\n",
      "Karim Khayrat:  But version one didn't do that. They just looked at about probability. So probably there were they would get a better score if they did that and version one and, and kept the open AI API as this. I'm just speculating here but it's not like a fair comparison, of course, like if you want to compare open source versus close source, This step I think is is quite important. So maybe this this step using gptj brought down the scores down but this step fixed it. Bit by providing higher quality filtering.\n",
      "Amir Feizpour: So, again, like I don't understand this table, the rows are different data sets.\n",
      "Karim Khayrat:  Yeah, the the every rose, a different dataset.\n",
      "Amir Feizpour:  Okay. So how do I read this? So they, they did a retrieval with Bm25 then fine-tuned with Marco. And that's a performance. They're getting after that.\n",
      "Karim Khayrat:  This is BM 25. Pure beam 25. And this is the\n",
      "Karim Khayrat:  ndcj of the top 10. I think maybe Suhas is better to explain this metric. That they're, they're reporting.\n",
      "Amir Feizpour: Ation. Okay. So the metric is a separate question.\n",
      "Karim Khayrat:  If? Yeah.\n",
      "Amir Feizpour: What I'm curious about is What's the difference between? Like, what is the column that that is called Marco?\n",
      "Karim Khayrat:  Yeah, so mono t53b fine-tuned on on the Marco data set.\n",
      "Amir Feizpour: Okay, so, fine tuning on Marco and then tried on all of these data sets. These are the numbers. And then in courses added,…\n",
      "Karim Khayrat:  Yes.\n",
      "Amir Feizpour: I see.\n",
      "Karim Khayrat:  You. So on, on the average metric. You know, improves.\n",
      "Karim Khayrat:  You know. Marginally. So\n",
      "Amir Feizpour: but, Yeah, it looks like there is no advantage in adding imports and imports too. Is there\n",
      "Karim Khayrat: But, I'm sorry. So I'm, I'm not familiar with how this small of a difference makes in practice. but, Yeah, there's\n",
      "Karim Khayrat:  that doesn't seem that way.\n",
      "Amir Feizpour: Okay.\n",
      "Karim Khayrat:  Yeah, so so yeah. Does anyone else have any questions?\n",
      "Karim Khayrat:  Otherwise, you can move on to the next paper.\n",
      "Karim Khayrat:  Okay, so I'm gonna stop sharing this tab. I'm going to start another one with with Self-instruct.\n",
      "Karim Khayrat:  Yeah.\n",
      "Karim Khayrat: Okay, so so this I'll give a top level overview. there's, so, for self-instruct, the goal is to generate And instruction data sets, synthetic instruction data, sets not for a particular. Instruction like the previous one was a particular instruction. Like, you, you given a document generate the question, right? Imagine that you want to generalize this. Given. Any task generate the prompt for that task so it's a lot more general. And basically what we've seen before with the document question is a subset of So, how does this work? So initially, It.\n",
      "00:30:00\n",
      "Karim Khayrat:  The data the the input to the to the process is 175 seat tasks which is like a reasonable number of seats. And every task it has one instruction and with one instruction, of course, and one instance per task, for example, you can say sort these numbers In ascending order. So that's the instruction. And then the instance per task, is the input and the output. So the input would be the array of numbers. You want to sort and the output would be the sort of and some and you'll see later that you know some tasks don't have an input, you just have an output.\n",
      "Karim Khayrat:  Like let's say right the letter to whatever or they can also be stated in us in in a four in a rigid form of you Know, instruction input output. Like right, a letter according to the following instructions and then the input would be You know through the school. And then the output will be the letter to the school. So you can they talk about this in the paper and detail and the the allow for both kinds of instructions. Let's say for more robustness So now you have a task tool. So this task pool consists of the tasks and then instruction and the instance per task. And this task tool is going to grow, so it starts with 175 but the aim is to grow it to bootstrap the model to generate more and more of this task pool. Okay, so the task pool, you give it to the language model.\n",
      "Karim Khayrat:  And it first step is instruction generation. So first step is to just generate instruction, not the input to not the output you instruction. So here's what the prompt looks like. So, come up with a series of tasks. That one. Instruction for existing task their student instruction class. So and so on. And then they ask the model, the the model to keep on generating and they stop at 16 tasks, but I guess they stop based on the limit for the output at that time. But yeah, you can potentially give it a larger problem and and more tasks. But again he there's also question of, you know, the cost of doing this with opening the I API. So yeah, so this instruction from so it's very cool. In a sense. Now you're just generating the instructions. You're not generating any input or\n",
      "Karim Khayrat: The next step. Will will skip over this classification. Task Identification parts Later. I'll explain why? If, if you're interested but basically you can look at the bottom task input outputs over here and the instruction here is, for example, give me a quote from a famous person on this topic. And you have that, you have the instance for the for the staff. So you have the input and output. And you're asking the model. To generate both the input and…\n",
      "Karim Khayrat's Presentation: You.\n",
      "Karim Khayrat: the output for the given instruction. So this is this is an interesting prompt. So, instant generation come up with examples for the following tasks. Try to generate multiple examples of possible in the task doesn't require additional input. You can generate the output directly. This is the, This is the prompt start of the prompt and i, The problem wouldn't have fit in the slides on breaking it down. So, this is like the top of the prompt example task, which exercises are best for you, reducing belly fat at home, and this that doesn't require any input. It's it's already, you know, gives the output directly But these are all good suggestions lying leg raises like in and out. Planks syrups pipeline sector.\n",
      "Karim Khayrat:  And then there's another task convert, 85, Fahrenheit to Celsius, and then there's another task sort, the given list sendingly, and over here, the problem. It's given in the prompts, you know, an example of generating two examples. Instead of one example. So here's example one, here's the input is output. Example, 2 input output.\n",
      "Karim Khayrat:  and here's another task, turn down a job offer by sending an email to a recruiter and here's the letter and then finally, The right before you give this prompt to GPT, you ask you the task and you input that you input one of the instructions. That you generated using the previous prompt over here. and then, so basically your bootstrapping, everything over here, your generating suctions, and then from the instructions that you generated your generating, these input output pairs, Yeah, and yeah. So do you do you have any questions till this point?\n",
      "00:35:00\n",
      "Amir Feizpour: And you repeat everything you said again, I'm kidding.\n",
      "Karim Khayrat:  Yeah, sure. So\n",
      "Amir Feizpour: It. Can you do summarize it because it was a little longer. So in case anybody lost the overall picture\n",
      "Karim Khayrat:  so, in summary, You you first step you generate instructions. You ask Gpt? Or any large language model to generate instructions. For you given a set of example, instructions. And then after you collect these instructions, you ask Chat GPT to. To generate the input and the output for these instructions.\n",
      "Karim Khayrat:  Right. And that way, you generate a lot of synthetic. Data. That. Overall includes. both the instruction and the tasks for those instructions for sorry, the input on the output for those instructions.\n",
      "Amir Feizpour: And the goal of this process is to create a data set for fine-tuning.\n",
      "Karim Khayrat:  Yes. So goal is of this process is to fine-tune A model that can then be used like, charge GPT. Like where you where you ask it for instructions. You give it some instructions and it follows your instructions.\n",
      "Amir Feizpour: I see. So the goal is to create an instruction. Fine-tuning this\n",
      "Karim Khayrat: Exactly. And now the filtering step. They try to filter to decrease the overlap between the similarity between the generated tasks. So you have a task, if it's very similar, let's say in the data set, they try to increase diversity of the data set. By filtering out. Filtering out tasks that are similar to each other.\n",
      "Karim Khayrat:  And yeah, so that's that's it at the top level, I think it's it's It's a more general data synthesis approach than the document question retrieval. Of course, you have different Aims in each one. But what I've, what I like about this one is that it actually\n",
      "Karim Khayrat:  Um it's it actually can generate the way they set things up together over here. You can actually generate, you know, And unlimited amount of training data by using another model. So don't, this is also something that's interesting is like if you have open AI now. You know, those everything is closed source for them, but you can query open. Yeah, let's say you have an infinite amount of budget. You can acquire you open AI and get it, you know, a killer data set and then train your own model. And you can get a similar performance to open AIs, you know, close source solution. So just because a company closed sources, Their, their data, but exposes the models via an API. You can still get, you know, use their model to to boost an existing model.\n",
      "Karim Khayrat:  and, I'll show a Web page now that that they use this approach to improve. Using charge gdpt, but in just trying some some some results. So what they did over here, they did a human human based score. So they're, they're human experts, which which involves five of the authors of the paper. And ranked the the outputs of the model as a B C or D. So A is correct and satisfying response. B is acceptable response with minor imperfection see response to the instructions but has significant errors and these completely and invalid or irrelevant response. So this is with vanilla gpt3. and,\n",
      "00:40:00\n",
      "Karim Khayrat:  gps3 plus t0 training, I forgot what this these This three. Will correspond to.\n",
      "Karim Khayrat:  And then there's the gpt3 self and self-instruct, which is what the paper, what this process that I showed here does. So You get a significant improvement? In performance. and they say it's comparable to, you know, stock GPT one, two, and three,\n",
      "Karim Khayrat:  And yeah. So so that's that's where the the that's where the left of at that point of the paper, But then,\n",
      "Karim Khayrat:  Let me.\n",
      "Karim Khayrat:  Self.\n",
      "Karim Khayrat: f*****.\n",
      "Karim Khayrat:  You first?\n",
      "Karim Khayrat:  okay, so that's where the left of the paper, but everything since then, there was a recent Recent development. I will share another tab like\n",
      "Karim Khayrat:  so,\n",
      "Karim Khayrat: yeah, so there was a recent development called Alpaca So they, they introduced alpaca 7 billion Model. 7 billion. Parameter model fine-tune using the llama 7B model. Which is open source. On 52k instructions, following demonstrations and these 52 instructions were generated using the self-instruct process.\n",
      "Karim Khayrat:  And they claimed that alpaca behaves qualitatively similar to opening eyes, text DaVinci, DaVinci 003 while being surprisingly small and easy and cheap to reproduce. So they they use a budget of less than $600. To, to do this. And they have a similar similar. Let me zoom in over here. So they have a similar, you know, similar process, the use text DaVinci and 175 Self-estruct CT tasks.\n",
      "Karim Khayrat:  And then they generated 52k instruction following examples. And some support advice fine-tuning of the meta. Model of the metas llama 7 billion model to get alpaca 7 billion.\n",
      "Karim Khayrat: Well, and The evaluation sorry about difficult, it's all it's it's a blog post. So there's not a lot of technical details and they say that they did some things differently. But here's the Web demo. Okay. And yeah, so they claim that this is as good as as DaVinci 0, 0 3. But from my, from My Um, for my experience, I query that with some something that I know about like, for example, I know about books on Food Dynamics, right? So I asked you to recommend books on introductory food dynamics and just gave me like books that I would never read but charge GPT gave me actually books that I have read in, in my academic year. So so\n",
      "Karim Khayrat:  So, and maybe their claim is, it's similar in. In standard. You know standard time more standard, or more widespread, more common tasks like writing a letter or doing or others, or summarizing a document or getting questions from document and so forth. But there's, there's still a limitation because you still have a relatively small model and it would inherit everything that the original lama 7 billion model has. So if so that's that's something that I noticed thing on it and yeah encourage you guys to play around with it. but yeah, like this is this, I think it's an exciting development in the sense that if you can, if you can, if you have a budget of $600, you can potentially, you know,\n",
      "Karim Khayrat:  create something that that similar to charge GPT. Fine tunes on your own documents. That's also, you know, and and the recipe is quite intuitive, you know, you just generate instructions and then documents that. Oh yeah.\n",
      "00:45:00\n",
      "Karim Khayrat: So running out of time, I guess. So you have any questions? I can go back to the slides.\n",
      "Amir Feizpour: I'm good. Anybody else questions?\n",
      "Amir Feizpour: Okay, if not something. So everyone. Thanks Karim for presentation. We will see you all next week. If you haven't decided what paper you will look at, but come back for\n",
      "Meeting ended after 00:45:36 👋\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "file_path = r'.\\transcripts\\eeq-qdde-agt (2023-03-17 12_11 GMT-4) - Transcript.docx'\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text.append(paragraph.text)\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "extracted_text = extract_text_from_docx(file_path)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcript_processing_functions import \\\n",
    "                        full_transcript2essay, \\\n",
    "                        extract_metadata_as_json, \\\n",
    "                        json2rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = full_transcript2essay(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large-Language models and machine learning research were the main topics of a presentation given by Karim Khayrat. The presentation discussed the use of large-language models in generating questions and the results of different experiments. Karim explained that the idea behind using large-language models is to improve the quality of generated questions. The presentation focused on a specific method called Guided by Bad Questions (GBQ), which aims to generate more complex and contextually aware questions. The GBQ method uses examples of bad questions from the MS Marco dataset and manually creates good questions. By using the full context of the document, the model generates both good and bad questions for each document.\\n\\nAmir Feizpour asked about the source of the questions used in the GBQ method. Karim clarified that the bad questions were taken from the MS Marco dataset, while the good questions were manually created. Nikhil Varghese added that good questions tend to have longer format answers compared to bad questions, which often have single-word answers.\\n\\nAnother question raised by Amir was about the process of filtering the questions. Karim explained that in the GBQ method, the questions were filtered based on the mono T5 model instead of probability. This step was more expensive but resulted in higher quality filtering.\\n\\nThe presentation also discussed the results of experiments conducted using different models and datasets. Karim compared the performance of the mono T5 model trained on the MS Marco dataset alone, the MS Marco dataset plus InPart V1, and the updated version of InPart V2. The scores did not change significantly between the different versions, indicating that the quality of the questions remained consistent.\\n\\nThe presentation included a table showing the performance of the models on different datasets. The metrics used in the table were not fully explained, but it was clear that the mono T5 model fine-tuned on the MS Marco dataset performed well across the different datasets.\\n\\nAmir asked for clarification on how to interpret the table. Karim explained that the table showed the performance of the models after retrieval with BM25 and fine-tuning with the MS Marco dataset. The metrics reported in the table were related to the top 10 results.\\n\\nIn conclusion, the presentation highlighted the use of large-language models in generating questions and the results of experiments conducted using different models and datasets. The GBQ method showed promise in improving the quality of generated questions, and the mono T5 model performed well across different datasets.\\n\\nIn another part of the presentation, Karim Khayrat discussed a paper on \"Few-Shot Learning for Information Retrieval Tasks\" and introduced the concept of self-instructs. The paper addresses the challenge of gathering expensive human data for training retrieval models. To overcome this, the authors propose a few-shot learning approach for generating synthetic training datasets.\\n\\nThe model overview involves providing a document and a prefix, which consists of N pairs of questions and relevant documents. The language model, such as GPT-3, is then used to generate a question that is relevant to the document. By setting N to three, thousands or even millions of training examples can be generated from a random document collection.\\n\\nHowever, not all generated questions are of equal quality. To filter them, the authors select the top k examples based on the log probability of the generated output. In the paper, they filtered 100,000 examples and then further narrowed it down to the top 10,000.\\n\\nKarim provides an example to illustrate the process. Given a document about the effects of caffeine during pregnancy, the language model is prompted to generate a question related to the document. The generated question is then paired with the document, and the top k pairs are selected based on their probability.\\n\\nAmir Feizpour asks several questions during the presentation. He asks about the setup for the language model G and whether it produces multiple questions or just one. Karim explains that in principle, multiple questions can be generated, but the paper mentions generating only one question per document. The examples used in the prompt are dynamically sampled, and there is no retrieval happening at this point.\\n\\nAmir also asks about the purpose of the re-ranker. Karim clarifies that the re-ranker is used to fine-tune the dataset. It takes the question and the document as input and scores their relevancy. The re-ranker is essentially a classifier that determines whether the question and document match or not.\\n\\nNikhil Varghese asks about the distinction between good and bad questions. Karim explains that in the prompt, they experimented with different methods, including having a good question and a bad question. A good question is relevant to the document, while a bad question is not. By prompting the language model with this distinction, they aim to generate higher quality questions.\\n\\nOverall, the presentation provides insights into the few-shot learning approach for information retrieval tasks and the use of self-instructs to generate synthetic training datasets. The method shows promise in reducing the need for expensive human data and improving the quality of generated questions.\\n\\nIn another part of the presentation, Karim Khayrat discussed the topic of large-language models and machine learning research. He began by mentioning that the presentation would cover the generation of instruction following examples and the fine-tuning of the meta-model of the Metas Llama 7 billion model to create the Alpaca 7 billion model.\\n\\nKarim then mentioned that the evaluation of the Alpaca 7 billion model was difficult to assess due to the lack of technical details in the blog post. However, he did mention that there was a web demo available. The claim made by the authors of the model was that it performed as well as the DaVinci 0.003 model. However, Karim expressed his skepticism based on his own experience. He mentioned that when he asked the model to recommend books on introductory food dynamics, it provided him with books that he would never read. In contrast, the Charge GPT model gave him recommendations that aligned with his academic background.\\n\\nKarim acknowledged that the Alpaca 7 billion model might perform well in standard tasks such as writing a letter, summarizing a document, or generating questions from a document. However, he pointed out that there were still limitations due to the relatively small size of the model and its reliance on the original Llama 7 billion model. This means that the Alpaca model inherits the strengths and weaknesses of its predecessor.\\n\\nDespite these limitations, Karim found the development of the Alpaca 7 billion model exciting. He highlighted that with a budget of $600, one could potentially create a model similar to Charge GPT by fine-tuning it on their own documents. He encouraged the audience to explore and experiment with the model.\\n\\nTowards the end of the presentation, Karim mentioned that time was running out and asked if there were any questions. Amir Feizpour, one of the attendees, stated that he had no questions. The meeting concluded with thanks to Karim for his presentation, and the attendees were reminded to come back for the next session without specifying the paper they would be discussing.\\n\\nIn conclusion, Karim Khayrat\\'s presentation provided insights into the Alpaca 7 billion model and its potential applications in machine learning research. He highlighted the limitations of the model while also acknowledging its exciting possibilities.\\n\\nLarge language models and machine learning research were the main topics of a presentation given by Karim Khayrat. The presentation focused on a specific paper called \"Self-instruct\" and discussed the goal of generating synthetic instruction datasets for various tasks.\\n\\nThe speaker explained that the previous paper they discussed involved generating questions based on a given document, which was a specific type of instruction. However, the goal of the \"Self-instruct\" paper was to generalize this process and generate prompts for any given task, making it more versatile.\\n\\nThe process started with a task pool consisting of 175 tasks, each with one instruction and one instance per task. For example, a task could be to sort a list of numbers in ascending order, with the instruction being the sorting process and the instance being the input and output. Some tasks didn\\'t require an input, only an output, such as writing a letter according to specific instructions.\\n\\nThe task pool was then given to a language model, which went through two steps: instruction generation and instance generation. In the instruction generation step, the model was asked to generate instructions based on existing tasks. The prompt for this step was to come up with a series of tasks and their corresponding instructions. The model generated multiple instructions, and the process stopped after 16 tasks were generated.\\n\\nIn the instance generation step, the model was asked to generate both the input and output for the given instructions. The prompt for this step asked the model to come up with examples for the tasks and generate multiple examples if possible. The generated examples included tasks like exercises for reducing belly fat, converting Fahrenheit to Celsius, and sorting a list.\\n\\nThe generated instructions and instances were then used to create a dataset for fine-tuning a model. The speaker mentioned that the goal of this process was to fine-tune a model that could follow instructions given to it, similar to OpenAI\\'s GPT model. The generated dataset was filtered to increase diversity and decrease overlap between similar tasks.\\n\\nThe speaker highlighted the advantages of this approach, including the ability to generate an unlimited amount of training data by using another model. They also mentioned a recent development called Alpaca, which introduced a 7 billion parameter model fine-tuned using the self-instruct process. This model was claimed to behave similarly to OpenAI\\'s text DaVinci model while being smaller and cheaper to reproduce.\\n\\nIn conclusion, the presentation discussed the self-instruct process for generating synthetic instruction datasets and its potential applications in fine-tuning language models. The speaker also mentioned the'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_filepath = r'./essays/babasaheb_essay.txt'\n",
    "# save the final essay to a file\n",
    "with open(essay_filepath, 'w') as file:\n",
    "    file.write(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_json = extract_metadata_as_json(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'Large-Language Models and Machine Learning Research',\n",
       " 'Speaker': 'Karim Khayrat',\n",
       " 'Summary': 'The presentation discussed the use of large-language models in generating questions and the results of different experiments. It also covered the self-instruct process for generating synthetic instruction datasets and the potential applications in fine-tuning language models.',\n",
       " 'Topics': [{'Topic': 'Guided by Bad Questions (GBQ) Method',\n",
       "   'Takeaways': ['Large-language models are used to improve the quality of generated questions.',\n",
       "    'The GBQ method uses examples of bad questions from the MS Marco dataset and manually creates good questions.',\n",
       "    'The model generates both good and bad questions for each document using the full context of the document.',\n",
       "    'Questions in the GBQ method are filtered based on the mono T5 model for higher quality filtering.']},\n",
       "  {'Topic': 'Results of Experiments',\n",
       "   'Takeaways': ['The performance of the mono T5 model trained on different datasets did not change significantly, indicating consistent question quality.',\n",
       "    'The mono T5 model fine-tuned on the MS Marco dataset performed well across different datasets.']},\n",
       "  {'Topic': 'Few-Shot Learning for Information Retrieval Tasks',\n",
       "   'Takeaways': ['The paper proposes a few-shot learning approach for generating synthetic training datasets.',\n",
       "    'The model generates questions relevant to a document by providing a document and a prefix.',\n",
       "    'The top examples are selected based on the log probability of the generated output.',\n",
       "    'The re-ranker is used to fine-tune the dataset by scoring the relevancy of the question and document pairs.']},\n",
       "  {'Topic': 'Alpaca 7 Billion Model',\n",
       "   'Takeaways': ['The Alpaca 7 billion model is a fine-tuned model using the self-instruct process.',\n",
       "    \"It performs similarly to OpenAI's DaVinci model but is smaller and cheaper to reproduce.\",\n",
       "    'The model has limitations due to its reliance on the original Llama 7 billion model.']}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "metadata_filepath = r'./metadata/babasaheb_sample.json'\n",
    "with open(metadata_filepath, 'w') as file:\n",
    "    json.dump(metadata_json, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert metadata from json to rst\n",
    "rst_filepath = r'./rst_files/babasaheb.rst'\n",
    "json2rst(metadata_json, rst_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
